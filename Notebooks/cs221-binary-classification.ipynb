{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8639065,"sourceType":"datasetVersion","datasetId":5153528}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":8.536944,"end_time":"2024-04-16T15:23:21.372397","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-16T15:23:12.835453","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries and prepare resources","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re, string\nimport numpy as np\n\nimport preprocessor as p","metadata":{"papermill":{"duration":0.74066,"end_time":"2024-04-16T15:23:16.378736","exception":false,"start_time":"2024-04-16T15:23:15.638076","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random, torch\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"papermill":{"duration":3.807783,"end_time":"2024-04-16T15:23:20.193595","exception":false,"start_time":"2024-04-16T15:23:16.385812","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"papermill":{"duration":0.020288,"end_time":"2024-04-16T15:23:20.220831","exception":false,"start_time":"2024-04-16T15:23:20.200543","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read dataset","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"path_train = \"/kaggle/input/dataset-cs221/train_en.csv\"\npath_val = \"/kaggle/input/dataset-cs221/val_en.csv\"\npath_test = \"/kaggle/input/dataset-cs221/test_en.csv\"","metadata":{"papermill":{"duration":0.016221,"end_time":"2024-04-16T15:23:20.244492","exception":true,"start_time":"2024-04-16T15:23:20.228271","status":"failed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the simple preprocessing\n# def preprocessing_text(text):\n#     text = text.strip()\n#     text = text.translate(text.maketrans('', '', string.punctuation.replace(\"_\",\"\")))\n#     text = re.sub('\\\\s+',' ',text).strip()\n#     return text","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Empty preprocessing fucntion because the dataset used is preprocessed\ndef preprocessing_text(text):\n    \n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def convert_label(text):\n    if text == \"Hope\":\n        return 1\n    else:\n        return 0\n\ndef read_and_preprocessing(path_data):\n    df = pd.read_csv(path_data)\n    df[\"binary\"] = df[\"binary\"].apply(convert_label)\n    x_input = df[\"text\"].apply(preprocessing_text).tolist()\n    y_output = df[\"binary\"].tolist()\n    ids = df[\"id\"].tolist()\n    return x_input,y_output,ids\n\ntrain_texts, train_labels,train_ids = read_and_preprocessing(path_train)\nvalid_texts,valid_labels,valid_ids = read_and_preprocessing(path_val)\nprint(len(train_texts),len(train_labels))\nprint(len(valid_texts),len(valid_labels))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.DataFrame(list(zip(train_texts, train_labels)),\n               columns =['x_data', 'y_output'])\ndf_train.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Filter Classifier","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name = \"FacebookAI/xlm-roberta-base\"\n\n# bert_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n# tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n\nbert_model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512, ignore_mismatched_sizes=True)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 512\ntrain_encodings = tokenizer(train_texts, truncation=True, max_length=max_length, padding=True)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = CustomDataset(train_encodings, train_labels)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=10,              # total number of training epochs\n    learning_rate=2e-5,              # 2e-5 for base models, 5e-5 for large model\n    per_device_train_batch_size=32,  # batch size per device during training\n    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=100,\n    save_total_limit = 1,\n    report_to=\"tensorboard\"\n)\n\ntrainer = Trainer(\n    model=bert_model,                         # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n)\ntrainer.train()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference (Validation datasets)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def make_prediction(review,tokenizer,trainer):\n    demo_input = preprocessing_text(review)\n    demo_encodings = tokenizer([demo_input], truncation=True, max_length = max_length, padding=True)\n    test_dataset = CustomDataset(demo_encodings, [0])\n    predic_demo = trainer.predict(test_dataset)[0]\n    predict_label = np.argmax(predic_demo, axis=1).flatten().tolist()[0]\n    return predict_label","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\n\nfor review in valid_texts:\n    a = make_prediction(review,tokenizer,trainer)\n    y_pred.append(a)\nprint(y_pred[:10])\nprint(valid_labels[:10])","metadata":{"_kg_hide-output":false,"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import *\n\nprint(\"M_Pr: \", round(precision_score(valid_labels, y_pred, average='macro'),4))\nprint(\"M_Re: \", round(recall_score(valid_labels, y_pred, average='macro'),4))\nprint(\"M_F1: \", round(f1_score(valid_labels, y_pred, average='macro'),4))\n\nprint(\"W_Pr: \", round(precision_score(valid_labels, y_pred, average='weighted'),4))\nprint(\"W_Re: \", round(recall_score(valid_labels, y_pred, average='weighted'),4))\nprint(\"W_F1: \", round(f1_score(valid_labels, y_pred, average='weighted'),4))\n\nprint(\"acc:\", round(accuracy_score(valid_labels, y_pred), 4))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions on Test datasets","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"test_texts, test_labels,test_ids = read_and_preprocessing(path_test)\n\ny_pred_test = []\n\nfor review in test_texts:\n    a = make_prediction(review,tokenizer,trainer)\n    y_pred.append(a)\nprint(y_pred_test[:10])\nprint(test_labels[:10])","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"M_Pr: \", round(precision_score(test_labels, y_pred_test, average='macro'),4))\nprint(\"M_Re: \", round(recall_score(test_labels, y_pred_test, average='macro'),4))\nprint(\"M_F1: \", round(f1_score(valid_labels, y_pred, average='macro'),4))\n\nprint(\"W_Pr: \", round(precision_score(test_labels, y_pred_test, average='weighted'),4))\nprint(\"W_Re: \", round(recall_score(test_labels, y_pred_test, average='weighted'),4))\nprint(\"W_F1: \", round(f1_score(test_labels, y_pred_test, average='weighted'),4))\n\nprint(\"acc:\", round(accuracy_score(test_labels, y_pred_test), 4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert back to label","metadata":{}},{"cell_type":"code","source":"def convert2category(y_pred):\n    y_label = []\n    for y in y_pred:\n        if y == 0:\n              y_label.append(\"Not Hope\")\n        else:\n              y_label.append(\"Hope\")\n    return y_label","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# name_sub = \"predictions.csv\"\n# y_pred_label = convert2category(y_pred_test)\n# df_sub = pd.DataFrame(list(zip(test_ids, y_pred_label)),\n#                columns =['id', 'category'])\n# df_sub.to_csv(name_sub)\n# df_sub.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]}]}