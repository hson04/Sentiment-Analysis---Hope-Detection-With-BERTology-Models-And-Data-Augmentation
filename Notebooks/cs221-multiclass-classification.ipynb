{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8639065,"sourceType":"datasetVersion","datasetId":5153528},{"sourceId":8809787,"sourceType":"datasetVersion","datasetId":5298882}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re, string\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random, torch\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_train = \"/kaggle/input/dataset-cs221-preprocessed/final_train_en.csv\"\npath_val = \"/kaggle/input/dataset-cs221-preprocessed/final_val_en.csv\"\npath_test = \"/kaggle/input/dataset-cs221-preprocessed/final_test_en_labeled.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read dataset","metadata":{}},{"cell_type":"code","source":"import string,re\ndef preprocessing_text(text):\n    text = text.strip()\n    text = text.translate(text.maketrans('', '', string.punctuation.replace(\"_\",\"\")))\n    text = re.sub('\\\\s+',' ',text).strip()\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def preprocessing_text(text):\n    \n#     return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef convert_label(text):\n    if text == \"Not Hope\":\n        return 0\n    elif text == \"Generalized Hope\":\n        return 1\n    elif text == \"Unrealistic Hope\":\n        return 2\n    elif text == \"Realistic Hope\":\n        return 3\n    else:\n        print(\"Error: \", text)\n        return 0\n    \n\ndef read_and_preprocessing(path_data):\n    df = pd.read_csv(path_data)\n    df[\"multiclass\"] = df[\"multiclass\"].apply(convert_label)\n    x_input = df[\"text\"].apply(preprocessing_text).tolist()\n    y_output = df[\"multiclass\"].tolist()\n    ids = df[\"id\"].tolist()\n    return x_input,y_output,ids\n\ntrain_texts, train_labels,train_ids = read_and_preprocessing(path_train)\nvalid_texts,valid_labels,valid_ids = read_and_preprocessing(path_val)\nprint(len(train_texts),len(train_labels))\nprint(len(valid_texts),len(valid_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.DataFrame(list(zip(train_texts, train_labels)),\n               columns =['x_data', 'y_output'])\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Filter Classifier","metadata":{}},{"cell_type":"code","source":"### import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n# model_name = \"FacebookAI/xlm-roberta-base\"\n\nbert_model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=4, ignore_mismatched_sizes=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512, ignore_mismatched_sizes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 512\ntrain_encodings = tokenizer(train_texts, truncation=True, max_length=max_length, padding=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = CustomDataset(train_encodings, train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=10,              # total number of training epochs\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,  # batch size per device during training\n    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=100,\n    save_total_limit = 1,\n    report_to=\"tensorboard\"\n)\n \ntrainer = Trainer(\n    model=bert_model,                         # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n)\ntrainer.train()\n    # API: fac85ccacc3dffb183116aba932e6bcc08010443","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference (with validation datasets)","metadata":{}},{"cell_type":"code","source":"def make_prediction(review,tokenizer,trainer):\n    demo_input = preprocessing_text(review)\n    demo_encodings = tokenizer([demo_input], truncation=True, max_length = max_length, padding=True)\n    test_dataset = CustomDataset(demo_encodings, [0])\n    predic_demo = trainer.predict(test_dataset)[0]\n    predict_label = np.argmax(predic_demo, axis=1).flatten().tolist()[0]\n    return predict_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ny_pred = []\n\nfor review in valid_texts:\n    a = make_prediction(review,tokenizer,trainer)\n    y_pred.append(a)\nprint(y_pred[:10])\nprint(valid_labels[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import *\n\nprint(\"M_Pr: \", round(precision_score(valid_labels, y_pred, average='macro'),4))\nprint(\"M_Re: \", round(recall_score(valid_labels, y_pred, average='macro'),4))\nprint(\"M_F1: \", round(f1_score(valid_labels, y_pred, average='macro'),4))\n\nprint(\"W_Pr: \", round(precision_score(valid_labels, y_pred, average='weighted'),4))\nprint(\"W_Re: \", round(recall_score(valid_labels, y_pred, average='weighted'),4))\nprint(\"W_F1: \", round(f1_score(valid_labels, y_pred, average='weighted'),4))\n\nprint(\"acc:\", round(accuracy_score(valid_labels, y_pred), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation on Test datasets","metadata":{}},{"cell_type":"code","source":"test_texts, test_labels,test_ids = read_and_preprocessing(path_test)\n\ny_pred_test = []\n\nfor review in test_texts:\n    a = make_prediction(review,tokenizer,trainer)\n    y_pred.append(a)\nprint(y_pred_test[:10])\nprint(test_labels[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import *\n\nprint(\"M_Pr: \", round(precision_score(test_labels, y_pred_test, average='macro'),4))\nprint(\"M_Re: \", round(recall_score(test_labels, y_pred_test, average='macro'),4))\nprint(\"M_F1: \", round(f1_score(test_labels, y_pred_test, average='macro'),4))\n\nprint(\"W_Pr: \", round(precision_score(test_labels, y_pred_test, average='weighted'),4))\nprint(\"W_Re: \", round(recall_score(test_labels, y_pred_test, average='weighted'),4))\nprint(\"W_F1: \", round(f1_score(test_labels, y_pred_test, average='weighted'),4))\n\nprint(\"acc:\", round(accuracy_score(test_labels, y_pred_test), 4))","metadata":{},"execution_count":null,"outputs":[]}]}